"""SFT Accounting Dataset Generator via NotebookLM.

Parses multi-part IFRS/IAS scenario questions from the question bank files,
sends each to NotebookLM (which has IFRS PDFs uploaded as sources), and
harvests the grounded answers for SFT dataset creation.

Usage:
  python sft-accounting-runner.py --notebook-id <UUID>
  python sft-accounting-runner.py --notebook-id <UUID> --start-at 50
  python sft-accounting-runner.py --notebook-id <UUID> --dry-run
  python sft-accounting-runner.py --post-process-only

The notebook must already have IFRS/IAS PDFs uploaded as sources.
"""
import json
import logging
import os
import re
import sys
import time
import argparse
import traceback
from pathlib import Path
from dataclasses import dataclass, field, asdict
from typing import Optional

# Add source to path so we can import directly
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

# ‚îÄ‚îÄ Logging Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LOG_DIR = Path.home() / ".notebooklm-mcp"
LOG_DIR.mkdir(exist_ok=True)
LOG_FILE = LOG_DIR / "sft-accounting.log"

logger = logging.getLogger("sft_accounting")
logger.setLevel(logging.DEBUG)

_fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
_fh.setLevel(logging.DEBUG)
_fh.setFormatter(logging.Formatter(
    "%(asctime)s [%(levelname)-7s] %(name)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
))
logger.addHandler(_fh)

_ch = logging.StreamHandler(sys.stderr)
_ch.setLevel(logging.INFO)
_ch.setFormatter(logging.Formatter("%(message)s"))
logger.addHandler(_ch)

# Also capture server logs
logging.getLogger("notebooklm_mcp.server").setLevel(logging.DEBUG)
logging.getLogger("notebooklm_mcp.server").addHandler(_fh)

# Force auth from cached tokens
os.environ.setdefault("NOTEBOOKLM_BL", "boq_labs-tailwind-frontend_20260212.13_p0")

# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
QUESTIONS_DIR = r"C:\PROJECTS\robsky-ai-vertex\SUPERVISED FINE TUNING"
OUTPUT_DIR = r"C:\PROJECTS\robsky-ai-vertex\SUPERVISED FINE TUNING\ACCOUNTING_RAW_ANSWERS"
SFT_OUTPUT_DIR = r"C:\PROJECTS\robsky-ai-vertex\SUPERVISED FINE TUNING"

QUESTION_FILES = [
    os.path.join(QUESTIONS_DIR, "NOTEBOOKLM_ACCOUNTING_QUESTIONS.md"),        # Q1-Q25
    os.path.join(QUESTIONS_DIR, "NOTEBOOKLM_ACCOUNTING_QUESTIONS_PART2.md"),  # Q26-Q75
    os.path.join(QUESTIONS_DIR, "NOTEBOOKLM_ACCOUNTING_QUESTIONS_PART3.md"),  # Q76-Q125
    os.path.join(QUESTIONS_DIR, "NOTEBOOKLM_ACCOUNTING_QUESTIONS_PART4.md"),  # Q126-Q155
]

# ‚îÄ‚îÄ Rate Limiting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
QUERY_DELAY_SECONDS = 15  # Delay between queries to avoid rate limits
MAX_RETRIES = 5           # Retry failed queries
BACKOFF_BASE = 30         # Base backoff on empty answer (doubles each retry)
COOLDOWN_AFTER_FAILURES = 60  # Cooldown seconds after consecutive failures
MAX_CONSECUTIVE_FAILURES = 3  # Trigger cooldown after N consecutive failures

# ‚îÄ‚îÄ Intent ‚Üí System Instruction Mapping ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# These map to the Robsky output format taxonomy from IMPLEMENTATION_PLAN_SFT_V2.md
SYSTEM_INSTRUCTIONS = {
    "COMPUTATION": (
        "You are a Senior CPA and IFRS specialist. You produce structured "
        "computation workbenches following this format: "
        "Inputs ‚Üí Formula/Standard Reference ‚Üí Step-by-Step Calculation ‚Üí "
        "Result ‚Üí Journal Entries ‚Üí Authority Citation. "
        "Show all intermediate steps. Use formal accounting prose. "
        "Every computation must trace back to the referenced IFRS/IAS standard."
    ),
    "RESEARCH": (
        "You are a Senior Partner at a top-tier accounting and advisory firm "
        "specializing in IFRS/IAS. You produce structured advisories following "
        "this format: BLUF (Bottom Line Up Front) ‚Üí Detailed Analysis ‚Üí "
        "Authority Hierarchy (IFRS/IAS references) ‚Üí Practical Implications ‚Üí "
        "Caveats. Use formal professional prose. Every assertion must be "
        "grounded in the referenced standards."
    ),
    "COMPARATIVE": (
        "You are a Senior IFRS Analyst. You produce comparative matrices "
        "following this format: Side-by-Side Comparison Table ‚Üí Detailed "
        "Analysis of Key Differences ‚Üí Practical Implications ‚Üí "
        "Recommendation. Use clear tabular formats where possible. "
        "Ground all comparisons in specific IFRS/IAS paragraphs."
    ),
    "INSTRUCTIONAL": (
        "You are an Accounting Professor and IFRS Expert. You produce "
        "procedural guides following this format: Prerequisites ‚Üí "
        "Step-by-Step Procedure ‚Üí Common Pitfalls ‚Üí Examples ‚Üí "
        "Standard References. Use clear, instructional prose. "
        "Ground all guidance in specific IFRS/IAS requirements."
    ),
    "COMPLIANCE": (
        "You are an IFRS Compliance Specialist. You produce compliance "
        "checklists following this format: Requirements ‚Üí Recognition "
        "Criteria ‚Üí Measurement Rules ‚Üí Disclosure Requirements ‚Üí "
        "Common Non-Compliance Issues ‚Üí Standard References. "
        "Use regulatory prose. Every requirement must cite specific "
        "IFRS/IAS paragraphs."
    ),
    "ANALYTICAL": (
        "You are a Financial Reporting Analyst specializing in IFRS. "
        "You produce analytical reports following this format: "
        "Key Metrics ‚Üí Impact Analysis ‚Üí Trend Implications ‚Üí "
        "Financial Statement Effects ‚Üí Standard References. "
        "Use data-driven, analytical prose."
    ),
    "PEDAGOGICAL": (
        "You are an Accounting Professor preparing tutorial materials. "
        "You produce tutorials following this format: Concept Overview ‚Üí "
        "Detailed Explanation with Examples ‚Üí Practice Scenarios ‚Üí "
        "Common Misconceptions ‚Üí Key Takeaways ‚Üí Standard References. "
        "Use pedagogical, accessible prose while maintaining technical accuracy."
    ),
}

# Default system instruction for questions that don't match a specific intent
DEFAULT_SYSTEM_INSTRUCTION = SYSTEM_INSTRUCTIONS["RESEARCH"]


# ‚îÄ‚îÄ Data Models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@dataclass
class ParsedQuestion:
    """A single parsed question from the question bank."""
    number: int                    # Q1, Q2, ..., Q155
    title: str                     # "Step Acquisition to Control (IFRS 3 + IFRS 10)"
    category: str                  # "BUSINESS COMBINATIONS & CONSOLIDATION"
    standards: list[str]           # ["IFRS 3", "IFRS 10"]
    background: str                # The scenario text
    sub_questions: list[str]       # ["a. How should...", "b. Calculate..."]
    full_text: str                 # The complete question as sent to NotebookLM
    source_file: str               # Which file it came from
    intent: str = "RESEARCH"       # Mapped Robsky intent


# ‚îÄ‚îÄ Question Parser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_questions_from_file(filepath: str) -> list[ParsedQuestion]:
    """Parse all questions from a single markdown file."""
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()

    questions = []
    current_category = ""

    # Split on question headers: ### Q1 ‚Äî Title
    # Use regex to find all question blocks
    q_pattern = re.compile(
        r'^### (Q(\d+))\s*[‚Äî‚Äì-]\s*(.+?)$',
        re.MULTILINE
    )

    matches = list(q_pattern.finditer(content))

    for i, match in enumerate(matches):
        q_label = match.group(1)       # "Q1"
        q_number = int(match.group(2)) # 1
        q_title = match.group(3).strip()  # "Step Acquisition to Control (IFRS 3 + IFRS 10)"

        # Extract the block between this match and the next (or end of file)
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        block = content[start:end].strip()

        # Find category from ## headers above this question
        # Look backwards from the question position for the last ## header
        cat_pattern = re.compile(r'^## (.+?)$', re.MULTILINE)
        cat_matches = list(cat_pattern.finditer(content[:match.start()]))
        if cat_matches:
            current_category = cat_matches[-1].group(1).strip()
            # Clean up category (remove "continued" etc.)
            current_category = re.sub(r'\s*\(continued\)\s*', '', current_category)

        # Extract standards from title (things in parentheses like "IFRS 3 + IFRS 10")
        standards = []
        std_match = re.search(r'\(([^)]+)\)', q_title)
        if std_match:
            std_text = std_match.group(1)
            standards = [s.strip() for s in re.split(r'[+,&]', std_text)]

        # Split into background and sub-questions
        # Background is everything before "**Questions:**" or the first "a."
        bg_split = re.split(r'\*\*Questions?:\*\*', block, maxsplit=1)
        if len(bg_split) == 2:
            background = bg_split[0].strip()
            questions_text = bg_split[1].strip()
        else:
            # Try splitting at first "a." line
            a_match = re.search(r'^a\.\s', block, re.MULTILINE)
            if a_match:
                background = block[:a_match.start()].strip()
                questions_text = block[a_match.start():].strip()
            else:
                background = block
                questions_text = ""

        # Parse individual sub-questions (a., b., c., d., e.)
        sub_questions = []
        if questions_text:
            # Match lines starting with a letter followed by a period
            sq_pattern = re.compile(r'^([a-z])\.\s+(.+?)(?=\n[a-z]\.\s|\n---|\Z)', re.MULTILINE | re.DOTALL)
            for sq_match in sq_pattern.finditer(questions_text):
                sub_q = f"{sq_match.group(1)}. {sq_match.group(2).strip()}"
                sub_questions.append(sub_q)

        # If we couldn't parse sub-questions properly, just use the full questions text
        if not sub_questions and questions_text:
            sub_questions = [questions_text]

        # Determine intent based on sub-question content and category
        intent = _classify_intent(q_title, background, sub_questions, current_category)

        # Build the full query text to send to NotebookLM
        full_text = _build_query_text(q_number, q_title, background, sub_questions, standards)

        questions.append(ParsedQuestion(
            number=q_number,
            title=q_title,
            category=current_category,
            standards=standards,
            background=background,
            sub_questions=sub_questions,
            full_text=full_text,
            source_file=os.path.basename(filepath),
            intent=intent,
        ))

    return questions


def _classify_intent(title: str, background: str, sub_questions: list[str], category: str) -> str:
    """Classify a question into a Robsky intent based on its content."""
    combined = (title + " " + " ".join(sub_questions)).lower()

    # Computation: questions asking for calculations, journal entries, amounts
    computation_triggers = ["calculate", "compute", "journal entr", "what amount", "how much",
                           "determine the", "show the computation", "measurement", "what is the amount"]
    if any(t in combined for t in computation_triggers) and not any(
        t in combined for t in ["compare", "difference between", "contrast"]):
        return "COMPUTATION"

    # Comparative: questions asking for comparison
    comparative_triggers = ["compare", "contrast", "difference between", "how does.*differ",
                           "side-by-side", "versus", "vs", "which approach"]
    if any(t in combined for t in comparative_triggers):
        return "COMPARATIVE"

    # Instructional: step-by-step procedure questions
    instructional_triggers = ["how should", "step-by-step", "procedure", "what steps",
                              "how to account for", "walk through"]
    # Only classify as instructional if there's no strong computation signal
    if any(t in combined for t in instructional_triggers) and \
       not any(t in combined for t in computation_triggers):
        return "INSTRUCTIONAL"

    # Compliance: disclosure and compliance questions
    compliance_triggers = ["disclosure", "what are the requirements", "compliance",
                          "what must be disclosed", "regulatory"]
    if any(t in combined for t in compliance_triggers):
        return "COMPLIANCE"

    # Pedagogical: teaching/learning questions
    pedagogical_triggers = ["explain why", "what is the rationale", "concept",
                           "what does.*mean", "pedagogical"]
    if any(t in combined for t in pedagogical_triggers):
        return "PEDAGOGICAL"

    # Analytical: impact analysis
    analytical_triggers = ["impact", "effect on financial", "analyze", "implication"]
    if any(t in combined for t in analytical_triggers):
        return "ANALYTICAL"

    # Default: RESEARCH (the advisory format)
    return "RESEARCH"


def _build_query_text(q_number: int, title: str, background: str, sub_questions: list[str],
                      standards: list[str]) -> str:
    """Build the full query text to send to NotebookLM."""
    parts = []

    # Header with standard references for grounding
    if standards:
        parts.append(f"Based on {', '.join(standards)}, please answer the following scenario-based question.\n")

    # Background/scenario
    parts.append(f"**Scenario (Q{q_number}: {title}):**\n{background}\n")

    # Sub-questions
    parts.append("**Questions:**")
    for sq in sub_questions:
        parts.append(sq)

    # Instruction for comprehensive answer
    parts.append(
        "\nPlease provide a comprehensive, well-structured answer addressing "
        "each sub-question. Include specific paragraph references to the relevant "
        "IFRS/IAS standards. Show all calculations step-by-step where applicable."
    )

    return "\n".join(parts)


def parse_all_questions() -> list[ParsedQuestion]:
    """Parse questions from all 4 question files."""
    all_questions = []

    for filepath in QUESTION_FILES:
        if not os.path.exists(filepath):
            logger.warning(f"Question file not found: {filepath}")
            continue

        questions = parse_questions_from_file(filepath)
        logger.info(f"Parsed {len(questions)} questions from {os.path.basename(filepath)}")
        all_questions.extend(questions)

    # Sort by question number
    all_questions.sort(key=lambda q: q.number)

    return all_questions


# ‚îÄ‚îÄ Cookie Refresh via Chrome DevTools Protocol ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#
# Strategy (mirrors notebooklm-mcp-runner.py):
#   1. Launch Chrome with --remote-debugging-port=9222 automatically
#   2. Navigate to notebooklm.google.com (keeps Google session alive)
#   3. Every REFRESH_INTERVAL_SECONDS, reach into Chrome via CDP and
#      extract fresh cookies ‚Äî Google refreshes PSIDTS automatically
#   4. Fallback: disk-based reload (for externally-saved cookies via MCP)
#
# This eliminates the need to manually paste cookies every ~20 minutes.

# Track auth.json modification time for auto-reload
_auth_file_mtime: float = 0.0
_last_refresh_time: float = 0.0
_chrome_launched: bool = False

# Auto-refresh settings (same as notebooklm-mcp-runner.py)
REFRESH_INTERVAL_SECONDS = 600  # Re-extract cookies every 10 minutes
REFRESH_INTERVAL_QUERIES = 20   # Or every 20 queries, whichever comes first


def _get_auth_file_path() -> str:
    """Get the path to the auth.json cache file."""
    return str(Path.home() / ".notebooklm-mcp" / "auth.json")


def ensure_chrome_cdp() -> bool:
    """Ensure Chrome is running with CDP on port 9222 and NotebookLM is loaded.

    This is the KEY function that prevents cookie expiration. Chrome keeps
    the Google session alive automatically ‚Äî we just need to reach in and
    grab the cookies periodically.

    Returns True if Chrome+CDP is ready.
    """
    global _chrome_launched

    from notebooklm_mcp.auth_cli import (
        CDP_DEFAULT_PORT,
        get_chrome_debugger_url,
        get_chrome_pages,
        launch_chrome,
        find_or_create_notebooklm_page,
    )

    # Check if Chrome is already running with CDP
    debugger_url = get_chrome_debugger_url(CDP_DEFAULT_PORT)

    if not debugger_url:
        if _chrome_launched:
            # We launched it before but it's gone ‚Äî don't re-launch automatically
            print("  [cdp] ‚ö†Ô∏è Chrome was closed ‚Äî CDP unavailable", file=sys.stderr)
            return False

        # Launch Chrome with our dedicated profile
        print("  [cdp] üöÄ Launching Chrome with remote debugging (port 9222)...")
        success = launch_chrome(CDP_DEFAULT_PORT, headless=False)
        if not success:
            print("  [cdp] ‚ùå Failed to launch Chrome", file=sys.stderr)
            return False

        _chrome_launched = True
        time.sleep(3)

        debugger_url = get_chrome_debugger_url(CDP_DEFAULT_PORT)
        if not debugger_url:
            print("  [cdp] ‚ùå Chrome launched but CDP not responding", file=sys.stderr)
            return False

    # Ensure NotebookLM page exists
    pages = get_chrome_pages(CDP_DEFAULT_PORT)
    nb_page = next((p for p in pages if "notebooklm.google.com" in p.get("url", "")), None)

    if not nb_page:
        print("  [cdp] üìÑ Opening NotebookLM page...")
        nb_page = find_or_create_notebooklm_page(CDP_DEFAULT_PORT)
        if not nb_page:
            print("  [cdp] ‚ùå Failed to open NotebookLM page", file=sys.stderr)
            return False
        time.sleep(3)  # Wait for page to load

    print(f"  [cdp] ‚úÖ Chrome ready (page: {nb_page.get('title', 'NotebookLM')})")
    return True


def reload_cookies_from_disk(force: bool = False) -> bool:
    """Re-read cookies from auth.json and reset the client singleton.

    Secondary refresh mechanism ‚Äî picks up cookies saved externally
    (e.g., via MCP save_auth_tokens).

    Args:
        force: If True, always reload. If False, only reload if file changed.

    Returns True if cookies were successfully reloaded.
    """
    global _auth_file_mtime

    try:
        auth_path = _get_auth_file_path()
        if not os.path.exists(auth_path):
            logger.warning("[disk-reload] auth.json not found")
            return False

        current_mtime = os.path.getmtime(auth_path)

        if not force and current_mtime == _auth_file_mtime:
            return False  # File hasn't changed

        # File changed ‚Äî reload
        from notebooklm_mcp.server import reset_client
        reset_client()
        _auth_file_mtime = current_mtime

        print(f"  [disk-reload] ‚úÖ Cookies reloaded from auth.json", file=sys.stderr)
        logger.info(f"[disk-reload] Reloaded cookies (mtime={current_mtime})")
        return True
    except Exception as e:
        logger.error("[disk-reload] Failed", exc_info=True)
        print(f"  [disk-reload] ‚ö†Ô∏è Failed: {e}", file=sys.stderr)
        return False


def check_and_reload_cookies() -> bool:
    """Check if auth.json has been updated and reload if so.

    Called before each query to auto-detect when fresh cookies
    are saved externally (e.g., via MCP save_auth_tokens).
    """
    return reload_cookies_from_disk(force=False)


def silent_refresh_cookies() -> bool:
    """Extract fresh cookies from Chrome via CDP.

    This is the PRIMARY refresh mechanism. Chrome keeps the Google
    session alive, so cookies extracted via CDP are always fresh.

    Falls back to disk reload if CDP is unavailable.
    """
    global _last_refresh_time

    try:
        from notebooklm_mcp.auth_cli import (
            get_chrome_pages, get_page_cookies, get_page_html, CDP_DEFAULT_PORT,
        )
        from notebooklm_mcp.auth import (
            AuthTokens, extract_csrf_from_page_source, save_tokens_to_cache, validate_cookies,
        )

        pages = get_chrome_pages(CDP_DEFAULT_PORT)
        nb_page = next((p for p in pages if "notebooklm.google.com" in p.get("url", "")), None)

        if not nb_page:
            # Try to launch/reconnect Chrome
            if ensure_chrome_cdp():
                pages = get_chrome_pages(CDP_DEFAULT_PORT)
                nb_page = next((p for p in pages if "notebooklm.google.com" in p.get("url", "")), None)

        if not nb_page:
            print("  [auto-refresh] No NotebookLM page found in Chrome", file=sys.stderr)
            return reload_cookies_from_disk(force=True)

        ws_url = nb_page.get("webSocketDebuggerUrl")
        if not ws_url:
            print("  [auto-refresh] No WebSocket URL for page", file=sys.stderr)
            return reload_cookies_from_disk(force=True)

        # Extract cookies from Chrome's live session
        cookies_list = get_page_cookies(ws_url)
        cookies = {c["name"]: c["value"] for c in cookies_list}

        if not validate_cookies(cookies):
            print("  [auto-refresh] Missing required cookies", file=sys.stderr)
            return reload_cookies_from_disk(force=True)

        # Extract CSRF token from page HTML
        html = get_page_html(ws_url)
        csrf_token = extract_csrf_from_page_source(html) or ""

        # Extract session ID
        from notebooklm_mcp.auth_cli import extract_session_id_from_html
        session_id = extract_session_id_from_html(html)

        # Save fresh tokens to cache
        tokens = AuthTokens(
            cookies=cookies,
            csrf_token=csrf_token,
            session_id=session_id,
            extracted_at=time.time(),
        )
        save_tokens_to_cache(tokens, silent=True)

        # Force the server module to reload its client
        from notebooklm_mcp import server
        server.reset_client()

        _last_refresh_time = time.time()
        print(f"  [auto-refresh] ‚úÖ Cookies refreshed via CDP "
              f"({len(cookies)} cookies, CSRF={'yes' if csrf_token else 'no'})")
        return True

    except Exception as e:
        logger.error("[auto-refresh] Cookie refresh failed", exc_info=True)
        print(f"  [auto-refresh] ‚ö†Ô∏è CDP failed: {e}", file=sys.stderr)
        # Fall back to disk reload
        return reload_cookies_from_disk(force=True)


def should_refresh(queries_since_refresh: int) -> bool:
    """Check if it's time for a proactive cookie refresh."""
    if queries_since_refresh >= REFRESH_INTERVAL_QUERIES:
        return True
    if _last_refresh_time > 0 and (time.time() - _last_refresh_time) > REFRESH_INTERVAL_SECONDS:
        return True
    return False


def _auth_recovery_callback() -> bool:
    """Auth recovery callback registered with the server module.

    This is invoked BY pipeline threads when they detect auth expiration.
    It refreshes cookies from Chrome CDP and resets the server's client.
    Thread-safe coordination is handled by the server module ‚Äî only one
    thread at a time will invoke this callback.
    """
    print("\n  üîë [mid-flight auth recovery] Thread detected expired auth, refreshing...", file=sys.stderr)
    success = silent_refresh_cookies()
    if success:
        print("  üîë [mid-flight auth recovery] ‚úÖ Done ‚Äî resuming with fresh auth", file=sys.stderr)
    else:
        print("  üîë [mid-flight auth recovery] ‚ùå Could not refresh cookies", file=sys.stderr)
    return success


# ‚îÄ‚îÄ Query Execution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def query_notebooklm(notebook_id: str, question: ParsedQuestion, output_path: str) -> dict:
    """Send a question to NotebookLM and save the answer to disk.

    Returns a result dict with status, answer length, etc.
    """
    from notebooklm_mcp.server import get_client

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # Check for fresh cookies from disk before each attempt
            if attempt > 1:
                check_and_reload_cookies()
            
            client = get_client()
            result = client.query(notebook_id, query_text=question.full_text)

            if not result or not result.get("answer"):
                logger.warning(f"Q{question.number}: Empty answer (attempt {attempt}/{MAX_RETRIES})")
                if attempt < MAX_RETRIES:
                    # Exponential backoff: 30s, 60s, 120s, 240s
                    backoff = BACKOFF_BASE * (2 ** (attempt - 1))
                    logger.info(f"Q{question.number}: Backing off {backoff}s before retry")
                    print(f"Q{question.number}: Backing off {backoff}s before retry", file=sys.stderr)
                    time.sleep(backoff)
                    # Try CDP refresh first, fall back to disk
                    silent_refresh_cookies()
                    continue
                return {"status": "error", "error": "Empty answer", "q_number": question.number}

            answer = result["answer"]

            # Save to disk
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                # Write metadata header
                f.write(f"# Q{question.number} ‚Äî {question.title}\n")
                f.write(f"**Category:** {question.category}\n")
                f.write(f"**Standards:** {', '.join(question.standards)}\n")
                f.write(f"**Intent:** {question.intent}\n")
                f.write(f"**Source File:** {question.source_file}\n")
                f.write(f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("---\n\n")
                f.write("## Question\n\n")
                f.write(question.full_text)
                f.write("\n\n---\n\n")
                f.write("## Answer\n\n")
                f.write(answer)

            return {
                "status": "success",
                "q_number": question.number,
                "answer_length": len(answer),
                "output_path": output_path,
                "conversation_id": result.get("conversation_id"),
            }

        except Exception as e:
            logger.error(f"Q{question.number}: Query failed (attempt {attempt}/{MAX_RETRIES})",
                        exc_info=True)
            if attempt < MAX_RETRIES:
                if "expired" in str(e).lower() or "auth" in str(e).lower():
                    # Auth error ‚Äî try CDP refresh immediately
                    print(f"  üîë Auth error on Q{question.number} ‚Äî refreshing via CDP...", file=sys.stderr)
                    silent_refresh_cookies()
                else:
                    time.sleep(QUERY_DELAY_SECONDS * 2)
            else:
                return {"status": "error", "error": str(e), "q_number": question.number}

    return {"status": "error", "error": "Max retries exceeded", "q_number": question.number}


def get_output_path(q_number: int) -> str:
    """Get the output file path for a given question number."""
    return os.path.join(OUTPUT_DIR, f"Q{q_number:03d}.md")


def is_answer_valid(filepath: str) -> bool:
    """Check if an answer file exists and is well-formed."""
    try:
        if not os.path.exists(filepath):
            return False
        size = os.path.getsize(filepath)
        if size < 300:  # Minimum viable answer
            return False
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        # Must have both Question and Answer sections
        return "## Question" in content and "## Answer" in content and len(content) > 500
    except Exception:
        return False


# ‚îÄ‚îÄ Post-Processing to SFT JSONL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_raw_answer(filepath: str) -> Optional[dict]:
    """Parse a raw answer .md file into structured data."""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        # Extract metadata from header
        metadata = {}
        for line in content.split("\n")[:10]:
            if line.startswith("**") and ":**" in line:
                key = line.split(":**")[0].replace("**", "").strip()
                value = line.split(":**")[1].strip()
                metadata[key.lower()] = value

        # Extract question and answer sections
        parts = content.split("## Answer")
        if len(parts) < 2:
            return None

        q_section = parts[0]
        answer = parts[1].strip()

        # Extract the question text (between "## Question" and "---")
        q_parts = q_section.split("## Question")
        if len(q_parts) < 2:
            return None
        question_text = q_parts[1].strip().rstrip("---").strip()

        return {
            "intent": metadata.get("intent", "RESEARCH"),
            "category": metadata.get("category", ""),
            "standards": metadata.get("standards", ""),
            "question": question_text,
            "answer": answer,
        }
    except Exception as e:
        logger.error(f"Failed to parse {filepath}: {e}", exc_info=True)
        return None


def convert_to_sft_jsonl(questions: list[ParsedQuestion], output_path: str):
    """Convert all raw answers to Vertex AI SFT JSONL format.

    Format matches existing sft_train.jsonl structure:
    {
        "systemInstruction": {"role": "system", "parts": [{"text": "..."}]},
        "contents": [
            {"role": "user", "parts": [{"text": "..."}]},
            {"role": "model", "parts": [{"text": "..."}]}
        ]
    }
    """
    examples = []
    missing = []

    for q in questions:
        raw_path = get_output_path(q.number)
        if not is_answer_valid(raw_path):
            missing.append(q.number)
            continue

        parsed = parse_raw_answer(raw_path)
        if not parsed:
            missing.append(q.number)
            continue

        # Get intent-appropriate system instruction
        system_instruction = SYSTEM_INSTRUCTIONS.get(q.intent, DEFAULT_SYSTEM_INSTRUCTION)

        # Build the SFT example
        example = {
            "systemInstruction": {
                "role": "system",
                "parts": [{"text": system_instruction}]
            },
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": parsed["question"]}]
                },
                {
                    "role": "model",
                    "parts": [{"text": parsed["answer"]}]
                }
            ]
        }
        examples.append(example)

    # Write JSONL
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for ex in examples:
            f.write(json.dumps(ex, ensure_ascii=False) + "\n")

    logger.info(f"Wrote {len(examples)} SFT examples to {output_path}")
    if missing:
        logger.warning(f"{len(missing)} questions had no valid answers: {missing}")

    return len(examples), missing


# ‚îÄ‚îÄ Progress Tracking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def save_progress(progress: dict, filepath: str = None):
    """Save progress to a JSON file for resume support."""
    if filepath is None:
        filepath = os.path.join(OUTPUT_DIR, "_progress.json")
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(progress, f, indent=2)


def load_progress(filepath: str = None) -> dict:
    """Load previously saved progress."""
    if filepath is None:
        filepath = os.path.join(OUTPUT_DIR, "_progress.json")
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"completed": [], "failed": [], "total_queries": 0}


def _format_eta(seconds: float) -> str:
    """Format seconds into human-readable duration."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        m, s = divmod(seconds, 60)
        return f"{int(m)}m {int(s)}s"
    else:
        h, remainder = divmod(seconds, 3600)
        m, _ = divmod(remainder, 60)
        return f"{int(h)}h {int(m)}m"


# ‚îÄ‚îÄ Main Entry Point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    parser = argparse.ArgumentParser(
        description="SFT Accounting Dataset Generator via NotebookLM"
    )
    parser.add_argument("--notebook-id", required=False,
                        help="NotebookLM notebook UUID (must have IFRS PDFs as sources)")
    parser.add_argument("--start-at", type=int, default=1,
                        help="Start processing from question N (default: 1)")
    parser.add_argument("--end-at", type=int, default=999,
                        help="Stop processing after question N (default: all)")
    parser.add_argument("--dry-run", action="store_true",
                        help="Parse and display questions without querying")
    parser.add_argument("--post-process-only", action="store_true",
                        help="Skip querying; convert existing raw answers to SFT JSONL")
    parser.add_argument("--delay", type=float, default=QUERY_DELAY_SECONDS,
                        help=f"Seconds between queries (default: {QUERY_DELAY_SECONDS})")
    parser.add_argument("--force", action="store_true",
                        help="Re-query even if valid answer already exists")

    args = parser.parse_args()

    # ‚îÄ‚îÄ Step 1: Parse all questions ‚îÄ‚îÄ
    print("üìñ Parsing question files...")
    questions = parse_all_questions()
    print(f"   Found {len(questions)} questions across {len(QUESTION_FILES)} files")

    # Show intent distribution
    intent_counts = {}
    for q in questions:
        intent_counts[q.intent] = intent_counts.get(q.intent, 0) + 1
    print(f"\n   üìä Intent Distribution:")
    for intent, count in sorted(intent_counts.items(), key=lambda x: -x[1]):
        print(f"      {intent}: {count}")

    # ‚îÄ‚îÄ Dry Run Mode ‚îÄ‚îÄ
    if args.dry_run:
        print(f"\nüîç Dry Run ‚Äî showing questions {args.start_at} to {min(args.end_at, len(questions))}:\n")
        for q in questions:
            if q.number < args.start_at or q.number > args.end_at:
                continue
            print(f"  Q{q.number:3d} | {q.intent:14s} | {q.category[:40]:40s} | {q.title[:50]}")
            print(f"        Standards: {', '.join(q.standards) or 'N/A'}")
            print(f"        Sub-questions: {len(q.sub_questions)}")
            print(f"        Query length: {len(q.full_text)} chars")
            out_path = get_output_path(q.number)
            exists = is_answer_valid(out_path)
            print(f"        Output: {'‚úÖ exists' if exists else '‚ùå missing'} ({out_path})")
            print()
        return

    # ‚îÄ‚îÄ Post-Process Only Mode ‚îÄ‚îÄ
    if args.post_process_only:
        print(f"\nüì¶ Post-processing {len(questions)} raw answers ‚Üí SFT JSONL...")
        sft_path = os.path.join(SFT_OUTPUT_DIR, "sft_accounting.jsonl")
        count, missing = convert_to_sft_jsonl(questions, sft_path)
        print(f"   ‚úÖ Wrote {count} examples to {sft_path}")
        if missing:
            print(f"   ‚ö†Ô∏è  {len(missing)} questions missing valid answers: {missing[:20]}...")
        return

    # ‚îÄ‚îÄ Query Mode ‚Äî requires notebook ID ‚îÄ‚îÄ
    if not args.notebook_id:
        print("‚ùå --notebook-id is required for querying. Use --dry-run or --post-process-only if you don't need it.")
        parser.print_help()
        sys.exit(1)

    notebook_id = args.notebook_id

    # ‚îÄ‚îÄ Chrome CDP Setup (auto cookie refresh) ‚îÄ‚îÄ
    global _auth_file_mtime, _last_refresh_time
    print("\nüç™ Setting up cookie auto-refresh via Chrome CDP...")
    print(f"   Auto-refresh: every {REFRESH_INTERVAL_QUERIES} queries or {REFRESH_INTERVAL_SECONDS}s")

    # Launch Chrome with CDP and do initial cookie extract
    if ensure_chrome_cdp():
        if silent_refresh_cookies():
            print("   ‚úÖ Fresh cookies loaded from Chrome")
        else:
            print("   ‚ö†Ô∏è Chrome running but cookie extract failed ‚Äî using cached cookies")
    else:
        # Fallback: load from disk
        auth_path = _get_auth_file_path()
        if os.path.exists(auth_path):
            _auth_file_mtime = os.path.getmtime(auth_path)
            print(f"   ‚ö†Ô∏è Chrome CDP unavailable ‚Äî using cached cookies from {auth_path}")
            print(f"   ‚ÑπÔ∏è  You can paste fresh cookies via MCP and workers will auto-detect them")
        else:
            print("   ‚ùå No Chrome CDP and no cached cookies ‚Äî queries will fail")

    # Register auth recovery callback (CDP first, disk fallback)
    from notebooklm_mcp.server import set_auth_recovery_callback
    set_auth_recovery_callback(_auth_recovery_callback)

    # Load progress for resume support
    progress = load_progress()
    completed_set = set(progress.get("completed", []))

    # Filter to requested range
    target_questions = [q for q in questions
                        if args.start_at <= q.number <= args.end_at]

    # Count existing valid answers
    already_done = sum(1 for q in target_questions
                       if is_answer_valid(get_output_path(q.number)))
    remaining = len(target_questions) - already_done if not args.force else len(target_questions)

    print(f"\nüöÄ Query Plan:")
    print(f"   Notebook: {notebook_id}")
    print(f"   Range: Q{args.start_at} ‚Üí Q{min(args.end_at, max(q.number for q in questions))}")
    print(f"   Total: {len(target_questions)} questions")
    print(f"   Already done: {already_done}")
    print(f"   Remaining: {remaining}")
    print(f"   Delay: {args.delay}s between queries")
    print(f"   Estimated time: {_format_eta(remaining * (args.delay + 15))}")
    print(f"   Output: {OUTPUT_DIR}")
    print()

    if remaining == 0 and not args.force:
        print("‚úÖ All questions already answered! Use --force to re-query.")
        print("   Run with --post-process-only to generate SFT JSONL from raw answers.")
        return

    # ‚îÄ‚îÄ Main Query Loop ‚îÄ‚îÄ
    batch_start = time.time()
    succeeded = 0
    failed = 0
    skipped = 0
    consecutive_failures = 0
    query_times = []
    refresh_counter = 0

    for i, q in enumerate(target_questions):
        out_path = get_output_path(q.number)

        # Skip if already done (unless --force)
        if not args.force and is_answer_valid(out_path):
            skipped += 1
            continue

        # ‚îÄ‚îÄ Proactive cookie refresh (prevents expiration) ‚îÄ‚îÄ
        refresh_counter += 1
        # Check for externally-saved cookies (disk watch)
        if check_and_reload_cookies():
            refresh_counter = 0
        # Proactive CDP refresh (time-based or query-count-based)
        elif should_refresh(refresh_counter):
            print(f"\n  ‚è∞ Proactive cookie refresh via CDP "
                  f"(after {refresh_counter} queries / "
                  f"{int(time.time() - _last_refresh_time)}s since last)...")
            if silent_refresh_cookies():
                refresh_counter = 0

        # Progress display
        done_count = succeeded + failed + skipped
        total_count = len(target_questions)
        elapsed = time.time() - batch_start

        if query_times:
            avg_time = sum(query_times) / len(query_times)
            eta = avg_time * (total_count - done_count)
            eta_str = _format_eta(eta)
        else:
            eta_str = "calculating..."

        print(f"  [{done_count + 1}/{total_count}] Q{q.number:3d} ‚Äî {q.title[:50]:50s} | ETA: {eta_str}")

        # Execute query
        q_start = time.time()
        result = query_notebooklm(notebook_id, q, out_path)
        q_elapsed = time.time() - q_start
        query_times.append(q_elapsed)

        if result["status"] == "success":
            succeeded += 1
            consecutive_failures = 0  # Reset on success
            answer_kb = result["answer_length"] / 1024
            print(f"         ‚úÖ {answer_kb:.1f} KB in {q_elapsed:.1f}s")

            # Update progress
            progress["completed"].append(q.number)
            progress["total_queries"] = progress.get("total_queries", 0) + 1
            if succeeded % 5 == 0:  # Save progress every 5 successes
                save_progress(progress)
        else:
            failed += 1
            consecutive_failures += 1
            print(f"         ‚ùå {result.get('error', 'Unknown error')} ({q_elapsed:.1f}s)")
            progress.setdefault("failed", []).append(q.number)

            # Extended cooldown after consecutive failures (rate limit likely)
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                cooldown = COOLDOWN_AFTER_FAILURES * (consecutive_failures // MAX_CONSECUTIVE_FAILURES)
                cooldown = min(cooldown, 300)  # Cap at 5 minutes
                print(f"         ‚è≥ {consecutive_failures} consecutive failures ‚Äî cooling down {cooldown}s...")
                time.sleep(cooldown)
                # Try CDP refresh, fall back to disk
                silent_refresh_cookies()
                continue  # Skip the normal delay

        # Rate limiting delay
        if i < len(target_questions) - 1:
            time.sleep(args.delay)

    # Final progress save
    save_progress(progress)

    # ‚îÄ‚îÄ Summary ‚îÄ‚îÄ
    total_elapsed = time.time() - batch_start
    print(f"\n{'='*60}")
    print(f"  üèÅ QUERY BATCH COMPLETE")
    print(f"  Succeeded: {succeeded}")
    print(f"  Failed: {failed}")
    print(f"  Skipped (existing): {skipped}")
    print(f"  Total time: {_format_eta(total_elapsed)}")
    if query_times:
        print(f"  Avg query time: {sum(query_times)/len(query_times):.1f}s")
    print(f"{'='*60}")

    # Auto post-process if all done
    total_valid = sum(1 for q in questions if is_answer_valid(get_output_path(q.number)))
    if total_valid == len(questions):
        print(f"\nüì¶ All {len(questions)} answers collected! Auto-generating SFT JSONL...")
        sft_path = os.path.join(SFT_OUTPUT_DIR, "sft_accounting.jsonl")
        count, missing = convert_to_sft_jsonl(questions, sft_path)
        print(f"   ‚úÖ Wrote {count} examples to {sft_path}")
    else:
        print(f"\nüìä {total_valid}/{len(questions)} answers collected so far.")
        print(f"   Run --post-process-only when all questions are answered.")


if __name__ == "__main__":
    logger.info("SFT Accounting Runner started, log: %s", LOG_FILE)
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Interrupted by user (Ctrl+C)")
        print("\n\nInterrupted! Progress saved. Re-run to resume.")
        sys.exit(1)
    except Exception as e:
        logger.critical("FATAL ERROR", exc_info=True)
        print(f"\nFATAL ERROR: {e}")
        traceback.print_exc()
        sys.exit(1)
